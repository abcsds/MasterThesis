# Emotion and Affect in Sentence Embedding

Exploring the representation of emotions in the abstract space generated by BERT and other sentence embedding models.

## Datasets

Several datasets are used. Most of them cannot be redistributed by me, so I only attach the folders and names of the orginial datasets. You can download them and place according to the structure in `.data/data.txt` and my scripts should be able to do the rest.

List of Datasets from sarnthil: https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets

## Models

The models used to embed sentences will either be downloaded and located in the `./models` folder, or used as a python library. In the first case, the model folder structure will be only visible in the `./models/models.txt` file, as per redistribution restrictions. The models used as python libraries are referenced in this document, as well as in the `requirements.txt` file.

# Requirements

## Tensorflow

Using docker to keep dependencies under control:
 - Docker >=19.03
 - NVIDIA Container Toolkit (check with `docker run --gpus all --rm nvidia/cuda nvidia-smi`)
 - Docker Image: `tensorflow/tensorflow:2.1.0-gpu-py3-jupyter`

### Build image

For using the docker image one must build it within the system: `docker build -t bert .`

<!-- `docker run --gpus all -v $(pwd):/tf -it bert python embed.py`
`docker run --gpus all -p 8888:8888 -v $(pwd):/tf -it bert` -->

### Running the containers:

Generaly, a container can be run with the following command: `docker run [-it] [--rm] [-p hostPort:containerPort] tensorflow/tensorflow[:tag] [command]`

This means that for running jupyter notebooks, the following command can be used:

`docker run --gpus all -p 8888:8888 -it tensorflow/tensorflow:2.1.0-gpu-py3-jupyter`

## Torch

Tensorflow is a mess to setup on different environments, specially as time moves along, and different versions require different dependencies, or even different python versions. I am also working with (hydrogen)[https://atom.io/packages/hydrogen] on (Atom)[https://atom.io/], which contacts an ipython kernel and allows me to work on scripts as I create them.

For these reasons, and because it's relatively easier to setup, I'm using pytorch with python viurtalenv and virtualenvwrapper:

```shell
# inside the project folder
pip install --user virtualenv virtualenvwrapper
mkvirtualenv -a . -r requirements.txt thesis
```

for working with the atom python-ide
`python -m pip install 'python-language-server[all]'`


## Cuda driver

I work on a desktop computer running Linu

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Oct_23_19:24:38_PDT_2019
Cuda compilation tools, release 10.2, V10.2.89
```

## Environment description

With the goal of mantaining reproductability I only use dockers or virtual environments. Those run on a Linux desktop:

```
OS: Manjaro Linux x86_64 ðŸ’ª
Kernel: 5.4.23-1-MANJARO
Shell: zsh 5.8
Resolution: 1920x1080
CPU: Intel i7-8700K (12) @ 4.700GHz
GPU: NVIDIA GeForce GTX 1080 Ti
Memory: 15938MiB
```

# Personal Notes
BERT
fasttext: https://github.com/facebookresearch/fastText/tree/master/python
glove: https://nlp.stanford.edu/projects/glove/
 -https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db
