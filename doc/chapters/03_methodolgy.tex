%!TEX root = ../thesis.tex
\chapter{Methodology}\label{chap:Methodology}
% Justification for the methodology

To analyze the representation of emotions in different word embeddings, this project has been divided in two main parts: Embedding and Analysis. The embedding part includes selecting the intermediate representation of the dataset, and the usage of the language model to do so. The Analysis is focused on finding the information contained in the language models, through the exploration of the mentioned intermediate representation. A high emphasis on dimensionality-reducing visualizations was done in this last part. These allow for the development of intuitions that can be further explored through statistical tests.

The process for finding information on the desired embedded structure has been divided in consecutive steps, that represent progressive steps into finding structure in a dataset. The steps are the following:

\begin{enumerate}
  \item \textbf{Correlation}: A correlation between embedded dimensions and labels.
  \item \textbf{Linear Transformation}: A correlation between the labels and a linear transformation of the embedded dimensions.
  \item \textbf{Non-Linear Transformation}: A non-linear transformation for obtaining separable clusters.
  \item \textbf{Hierarchical clusterinng}: Clustering the embedded dimensions, or linear transformations of these.
\end{enumerate}

These steps also correspond to the complexity of a theoretical classifier on the embedded dimension. The first one answering to the question: Can the output of the embeddings be used directly to classify the labels? The second step corresponds to asking if a linear transformation of the embedded space could yield a classifier. For example, can an LDA preform on this dataset and embedding? The third question relates to the preformance of a non-linear classifier. This is the case of a Single Layer Perceptron. Considering the embeddings can be used as an intermediate step, for a further clasification, this step should already yield the results of baseline emotion classifiers. The fourth question relates to the relation of emotion and valence. Emotions tend to fit hierarchicaly into affective models of arousal and valence~\cite{barradas2016thesis}. By searching for a hierarchy in the representation of emotions, we should be able to trace back the representation of valence.

Separating the methodology in this way, enables a progressive apporximatons approach to answering the research question. It is highly unlikely that a general lenguage model in machine learning represents emotions into a single dimension in a linear manney, but it is increasingly more likely that some correlation is found with a linear transformation of the aforementioned. In case these two approaches present no information about emotions, a hierarchical clustering can extract the intrinsic information of affect in emotions. Since previous works have already shown that affect can be represented in vector spaces, created with a linear transformation of word embeddings~\cite{hollis2016principals}, it would be contradictory to not find a hierarchical structure of emotions in this last step. If this were to happen, it would be reasonable to question the dataset and it's methodology, or the contextual information lost in the embedding process.

\section{Preliminaries}\label{sec:Preliminaries}
This research was managed as both, a research project, and a software development project.
With scientific rigor, order, and reproducibility in mind, a git repository has been setup, where not only the working environment is provided, but also the history of the project  development.

\subsection{Environment Setup}\label{sub:Environment Setup}
The environment is all required hardware and software necessary to execute this computational experiment. Here it is presented how to reproduce the same environment, to be able to reproduce the results presented. A short organizational note is also included, to keep track of the project management.

\subsubsection{Organizational}\label{subs:Organizational}
The project planning was layed out throught three months: March, April and May of 2020. A total of twelve weeks were divided into four equal sprints, where the four main tasks in the project were equaly separated in time: Exploration and Preparation, Programming, Experiments, and Writing. The four sprints were described by tasks, further divided by sub-tasks. These were kept in track and followed by me, and both Supervisors through the Asana application.

The repository is accessible through github: \url{https://github.com/abcsds/MasterThesis}

\subsubsection{Hardware}\label{subs:Hardware}
% The computer
This project was implemented and executed in my personal computer: A Manjaro Linux x86_64, with Kernel 5.6.11-1-MANJARO. The available CPU is an Intel i7-8700K (12) @ 5.000GHz, and the GPU is an NVIDIA GeForce GTX 1080 Ti. A total of 15937MB of RAM memory were available for experiments, as well as 16GB of swap disk. Although much of the technology available for these experiments is more than necesary, the execution of some BERT models is not possible with these technical specifications. This influenced the selection of the BERT model to be used, and played a big part in selecting a pre-embedding of models.

\subsubsection{Software}\label{subs:Software}
% The operating system
As mentioned, the development and execution were on a Linux Operating Systetm (OS). The Distribution used was Manjaro. This OS is a rolling release distribution, so the version used changed allong the development. This is one of the reasons why virtual development and execution environments were used: to keep reproductability, and ensure a stable testing. The only reason for this OS to be used is that it is my personal computer.
% The repo
As a Version Control System (VCS), git was added to the repository. This enables distributed access and historical revision for anyone trying to reproduce or supervise the project.
% The development environment
Several development tools were used. For text editing and script execution, Atom 1.46.0 was used. Within the Atom environment, community packages were used to simplify the workflow: Hydrogen 2.14.1, for example, allows the execution of python code from within the text editor, and can even show output of the lines executed. A list of the used packages is provided in~\ref{TODO}
For some exploratory analysis, Jupyter Notebooks~\cite{kluyver2016jupyter} were used. To run these, a specific virtual environment was created with Docker 19.03 and NVIDIA-Docker. A docker image for theses notebooks was created. The dockerfile of this image contains the libraries used for data exploration. The downloading of the BERT models ran in TensorFlow is also contained in this dockerfile. The description and an initialization script for the virtual container are included in the project folder called \"TF\".
% How was the day to day? Explain on atom and script running
While the notebooks provided were used for data exploration, and visualization. Most of the development was done on the text editor. For this, python virtual environments were created with the help of the \lstinline{virtualenv} and \lstinline{virtualenvwrapper} python libraries. For these, a \"requirements.txt\" file was provided with the libraries used, and their versions.
% The development environment
When developping, the desired virtualenvironment was activated. After this, the atom editor is open on the desired folder. By doing so, the Hydrogen library takes the virtual environment for the execution of the code in the project.
By developing in this way, the whole project is available from the folder view on Atom. Code can be executed, and tested on the run, as if it were a Jupyter notebook, but changes are inmediately integrated into the code repository.
% Justification
This specific development environment was seleted to avoid conflicts between Jupyter Notebooks, and the VCS. The explorations are stored as notebooks, but cannot really represent the development of the project.
% The programming language
The Python programming language was used for the programming of the current project. This is due to it's incredible flexibility, access to the main ML libraries, and the predisposition of the Wirtschaftsinformatik und Maschinelles Lernen Institut. Under Python's umbrella of libraries, several were specifically added to enable this study. A List of the used libraries is provided in the appendix~\ref{TODO}.

% The ML frameworks
Two main ML frameworks were selected for the current project: TensorFlow 2.1.0~\cite{tensorflow2015whitepaper} (TF), and PyTorch 1.4.0~\cite{pytorch2019} (Also called Torch, for simplicity.). TF was selected specifically for it's access to a pre-trained BERT library~\cite{lai2015bertembedding} for embedding sentences. This was very usefull, since, compared to the Transformers library~\cite{wolf2019huggingface}, it must not be fine-tunned. TF confronts developers with two main compatibility issues:

\begin{itemize}
  \item The cuda library being used most be a specific version. Most TF libraries will only work under CUDA library 9.2. Some might run under 10.1, but not under 10.2. Since the development environment is a rolling release linux distribution, the latest version of libraries is provided. Installing multiple versions brings problems to the day-to-day usage. Since the environment is also my personal computer, a virtual environment with containers were used instead, and for these, NVIDIA-Docker.

  \item at the moment of the development of this project, TF is undergoing a major version change, from 1.x to 2.x. Many reference libraries, and all code I have creted, used, or studied in my masters is depricated. The techniques learned during my studies need to be updated, and in many cases, re-learned.
  This is not an uncommon problem in technology, but it opens the opportunity for changing the work framework.
\end{itemize}

For all ML programming requirements that did not use the pre-trained BERT library, PyTorch was used. Certain algorithms were not programmed, but simply integrated from their implementation on python:

\begin{itemize}
  \item FastText: This algorithm was not implemented. It's python library from the implementaiton of Facebook Research was used~\cite{joulin2017fasttext}.
  \item MulticoreTSNE: The TSNE algorithm was not implemented. Since it has heavy requirements on hardware, its implementation using distributed computing was used~\cite{ulyanov2016tsne}.
  \item Normalize: The sklearn version of the normalization algorithm was used due to its optimization~\cite{sklearn}.
  \item PCA: SKlearn version was used~\cite{sklearn}.
  \item Tokenization: Part of the embedding pipeline requires the tokenization of the sentences. This was done with the Spacy library, and the "en_core_web_sm" model.\cite{spacy}
\end{itemize}

\subsection{The Datasets}\label{sub:The Datasets}
Three datasets were selected to be used for this project. Here it's described how and when they were accessed, stored and embedded into the intermediate representation.

\subsubsection{Access}\label{subs:Access}
Accessing datasets to train machine learning models is not a standarized process. The developer of every dataset is in charge of the distribution method. Fortunately, two of the three datasets used in this project were distributed by the same organization: the EmotionPush, and Friends datasets were, while CrowdFlower was distributed originally by a company with the same name.

The CrowdFlower dataset was downloaded from the official CrowdFlower website in October 2019. The url to this dataset is \url{http://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv}. As of May 20, 2020, this link still works, but the website \url{www.CrowdFlower.com} redirects to \url{www.appen.com} a company that \"collects [data] to build [\ldots] artificial intelligence systems.\" This company offers access to some open source datasets, but the mentioned crowdflower emotion dataset is not listed there. A discussion on this is provided on chapter \ref{chap:05_conclusion}.

The EmotionPush and Friends datasets were distributed as part of the EmotionX Task, which in turn is part of a set of Social NLP tasks, created by the Academia Sinica of Taiwan~\cite{chen2018emotionlines}. To access this datasets, one must register on the EmotionX 2019 website: \url{https://sites.google.com/view/emotionx2019}. Access to a google drive is then granted via email, and a zip file with both datasets can be downloaded. This dataset has been used more than once in different analysis on the internet, and it can be therefore accessed without permissions to the official method. Here, the original dataset is used.

\subsubsection{Storage}\label{subs:Storage}
The datasets were downloaded and stored under the project folder \"data\". Since every dataset is provided in different format and under different folder structures, every dataset is simply stored inside a folder with it's name.
Under the datasets folder, every selected dataset is accompanied by folders with the embedding model used to embed the dataset. Thus every dataset folder has several subfolders. On these subfolders, a python script called \"embed.py\". This script varies for every model and dataset. In general terms, it extracts the text and label from the dataset, embeds the text into the desired model, and stores it in a \"csv\" file under the same folder.
The \"csv\" file is stored under the name \"embedded.py\", except for the FastText model. In this case, there are two embedding approaches, one supervised and one unsupervised. Thus the names of the FastText embedding files are \"embeddings_supervised.csv\", and \"embeddings_unsupervised.csv\". Every other script creates a single \"csv\" file called \"embeddings.csv\".

Thie file structure of the embedded files allows for exploration and experimental scripts to access the embedded data of different datasets, by building a single string with the dataset and model selected. This string must be prepended by the \"./data/\" folder name, and appended with the \"embeddings.csv\" string to generate a path that creates accesibility to the different datasets via a python coma-sepparated-value library, such as the built in \lstinline{csv}, or Pandas~\cite{reback2020pandas} and it's \lstinline{read_csv} function. This effectively create a data source to be used in a data pipeline. This approach was selected due to it's simplicity.

\section{Embedding}\label{sec:Embedding}
The comparisson of the representation of different language models in this project requires a convergence of many different techniques. For this reason it was chosen to embed the datasets into an intermediate format, to later use them in experiments.

The embedding of the datasets is comprised of 5 steps:

\begin{enumerate}
  \item Loading model, text, and labels.
  \item Tokenizing text.
  \item Embedding every token into the model latent space.
  \item Average the given embedded words.
  \item Store the average sentence embedding.
\end{enumerate}

\subsection{Embedding Methodology}\label{sub:Embedding Methodology}

Loading text, and labels was done with either the CSV or the JSON python library, depending on the format of the data.

Tokenizing was done with Spacy's \"en_core_web_sm\" model, which allows access to the tokens via an iterator on the model, and the sub-compontent \"text\". An small snippet showing this process is shown in~\ref{lst:spacy}. This snippet considers a model has been loaded as a dictionary on tokens.

\begin{lstlisting}[caption={Tokenizing with Spacy},label=lst:spacy,frame=single]
import spacy
nlp = spacy.load("en_core_web_sm")
for token in nlp("This is a sentence in English"):
  word_embedding = model[token.text]
\end{lstlisting}

Every token is embedded in this way, but some models might not contain some tokens. In this case, the token is simply skipped. Some tokens with relevant information can be lost with using pretrained models that don't contain the complete vocabulary of the dataset, but it is expected, that the information distribution converge to the real distribution when large number of samples are integrated.

Once every token has been embedded into the model's latent space. A simple average is done across the tokens, keeping the dimensionality of the vector representation, and effectively creating a sentence embedding, represented in the model's latent space. This technique was selected since it's the most common method for sentence representation. With this method, the secuential nature of the tokens in the sentence is lost, in favor of providing a constant sized sentence embedding to compare between methods and datasets.

Lastly, the sentence embeddings are stored along with the label information. For this, the CSV format was selected, due to it's interoperabiliy, and accessibility. The statistics library Pandas has an excellent csv reader, but the data can also be imported into spreadsheet software, other statistical software, or very quickl loaded on to python with the CSV library. Every CSV file contains a header on the first row. The header is composed by $N+1$ columns where $N$ is the number of latent dimensions in the model. The last column is the "Emotion" column, where the label is stored. The name of every column starts with the letter \"d\", and is followed by consecutive numbers.

Since every pre-trained model is different, there were specific requirements on loading the model and embedding the tokens:

\subsection{FastText}\label{sub:FastText}
As previously mentioned, the FastText algorithm is an exception in this project, since it is NOT a pretrained model. The model is trained based on the dataset given. This can be done in a supervised, or an unsupervised manner. Due to the two methods for the usage of the FastText python library, the process of embedding a dataset with it requires two extra text files one with a sentence per line, and a second one, which includes the label as the last word of every line, prepended by two underscores (\lstinline{__}).

In both ways of training, the language model is being trained specifically for the dataset vocabulary. For this reason, all tokens will be available in the model's vocabulary, resulting in the most complete language model. This is at the cost of representing only the topics on the dataset. This is therefore also not a general languge model.

\subsection{Word2Vec}\label{sub:Word2Vec}
Word2Vec is trained in a very similar way as fasttext. Therefore, the expected results are similar. Word2Vec is treated within the context of this experiments as the pre-trained equivalent of fast text. The same number of latent dimensions, and a similar training approach were used. In this case, if a word in the dataset is not contained in the Word2Vec model, it is droped, and its analysis won't be included in the results of this project. Word2Vec was trained with a very large corpus, it is therefore considered a general language model.

The pre-trained model has been stored under the project folder \"./models/Word2Vec/GoogleNews-vectors-negative300.bin.gz\". The gensim python library is used to load the model in binary format without having to decompress it. This model is loaded as a dictionary. An example of this is shown in snippet~\ref{lst:load_w2v} that considers an iterator over a tokenized sentence.

\begin{lstlisting}[caption={Loading Word2Vec},label=lst:load_w2v,frame=single]
import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)
for token in tokenized_sentence:
  word_embedding = model[token]
\end{lstlisting}

\subsection{GloVe}\label{sub:GloVe}
Pre-trained GloVe models can be downloaded from the official website \url{TODO}. This model was downloaded and stored under the project folder \"./models/GloVe/glove.6B/glove.6B.300d.txt\". The name of this file contains two numbers: 6B is the number of words that are represented in this model, while 300d is the number of latent dimensions used to represent the vocabulary. This model has been trained for 50, 100, 200, and 300 dimensions. Since a smaller number of dimensions represents a lesser capability for representing complex language concepts~\cite{penningto2014glove}, the larger version of this model was selected. This also concides with the number of dimensions used in Word2Vec, which makes results easier to compare.

\subsection{BERT}\label{sub:BERT}
Although BERT is a pretrained model, it's original distribution is considered to be only partialy trained. On the original paper~\cite{devlin2019bert}, a fine-tunning task-specific phase is mentioned, and generally required for the model to work best. This fine\-tunning also presents a great infrastructure challenge, since some pre-trained BERT models simply wont fit into a personal computer's RAM.\@

For this reason, the pre-trained BERT embedding library \url{https://github.com/imgarylai/bert-embedding} was used. This library allows for a selection of the BERT model, and the embedding of the whole sentence, without tokenization. The result is a json-like dictionary in Python that contains both the original sentence and the embedded sentence.

To be able to run the embedding notebook, provided under the project folder \"exploration/Embedding with bert.ipynb\", the following requirements should be met:
\begin{itemize}
  \item Docker $\geq19.03$
  \item NVIDIA Container Toolkit
  \item This Docker TF Image: \lstinline{tensorflow/tensorflow:2.1.0-gpu-py3-jupyter}
\end{itemize}

On Linux, the a correct installation of the nvidia-docker environment would yield a successful run of the following command: \lstinline{docker run --gpus all --rm nvidia/cuda nvidia-smi}

To build the docker image for this project, one must open a terminal on the \"TF\" project folder and run the following docker instruction: \lstinline{docker build -t bert .} where bert is the name of the image to be created.
Once this image has been built, docker can create containers with it. So to run the container necessary for the BERT embedding, the following command is used inside the project folder: \lstinline{docker run --gpus all -p 8888:8888 -v $(pwd):/tf -it bert}.
This last command will run a docker container, based on the "tensorflow:2.1.0-gpu-py3-jupyter" image, connect it to the localhost port 8888, and integrate the project folder to the jupyter server running on the container.

Docker is used to comply with the complex requirements of TensorFlow, CUDA, and the bert-embeddings.Once the Jupyter server is running, the notebook can be opened, and executed. The loading of the model is shown in the following snippet~\ref{lst:load_bert}:

\begin{lstlisting}[caption={Loading BERT},label=lst:load_bert,frame=single]
from bert_embedding import BertEmbedding
bert_embedding = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased')
\end{lstlisting}

Here, the selected model is shown. This is a model with 1024 latent dimensions, trained on the Wikipedia corpus, and with case sensitivity. This means that words lowercase and uppercase letters will be embedded differently.

Within the notebook, a function was created to embed the datasets. This receives three arguments: a list of the sentences, a list of the labels, and the name of the output file, as a string. The embedding function is shown here:

\begin{lstlisting}[caption={Embedding with BERT},label=lst:embed_bert,frame=single]
def embed_and_save(X, Y, outpath):
    E = np.array([np.mean(t[1], axis=0) for t in bert_embedding(X)])
    with open(outpath, 'w', newline='') as f:
        fieldnames = [f"d{i}" for i in range(len(E[0]))] + ['emotion']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for e, l in zip(E, Y):
            writer.writerow(dict({f"d{i}": ei for i, ei in enumerate(e)}, **{'emotion': l}))"
\end{lstlisting}

Running the embeddings for the datasets reported the following data:

\begin{table}[H]
  \begin{tabular}{lllll}
  Dataset                          & User              & System    & Total        & Wall        \\
  \hline
  \multicolumn{1}{l|}{CrowdFlower} & user 3h 57min 7s  & 13min 11s & 4h 10min 18s & 1h 5min 33s \\
  \multicolumn{1}{l|}{EmotionPush} & user 1h 28min 35s & 5min 25s  & 1h 34min 1s &  24min 31s   \\
  \multicolumn{1}{l|}{Friends    } & user 1h 26min 40s & 5min 3s   & 1h 31min 44s & 24min 9s
  \end{tabular}
  \caption{Runtimes for embedding datasets with BERT}\label{tab:rt_BERT}
\end{table}

This is much less than the \"several days\" verbaly reported by colegues at the ISMLL. This might be due to the use of pre-trained models, and not running back-propagation to fine-tune the language models.

While running the embeddings, almost no GPU memory was used. This signals that the library is actually not making use of the GPU resources available. This also might mean that the embedding of the datasets might be much faster if the correct hardware resources are used.

At the beginning of the Year 2020, the library seemed a reliable way of getting the embedding done quickly. It allowed for embedding of the complete datasets in matter of minutes. Since I had been warned BERT embeddings could take days, I saw this as a gread advantage, and kept the method. Unfortunately as of May 2020, this library has been depricated. It's unmantained, and has requirements that might only be achievable under very specific conditions. This will not be a problem for reproduction, as long as the library is still available, and the provided docker image is used.

\section{Analysis}\label{sec:Analysis}
The python scripts to analyze the data are found under the folder \"./exploration\", %TODO: Is the folder still the same?
where they are numbered, and named. The order of the scripts corresponds to the progressive steps in the search for structure in the embedded spaces. The scripts are the following:

\begin{enumerate}
  \item \lstinline{01_corr.py}
  \item \lstinline{02_pca.py}
  \item \lstinline{03_tsne.py}
\end{enumerate}

The order of these scripts corresponds to the methodology proposed in this thesis. They generate the visualizations, and test the hypotesis on the data. The last step in the methodology, Clustering, has been performed in all scripts. The scripts each contain a list of strings. Every string in that list is the relative path of one of the pre-processed datasets. These strings can be commented out. In doing so, the analysis will not be run on that specific instance. The full list is declared as follows:

\begin{lstlisting}[caption={Pre-processed datasets},label=lst:datasets,frame=single]
dss = ["data/CrowdFlower/FastText/embeddings_unsupervised.csv",
       "data/CrowdFlower/FastText/embeddings_supervised.csv",
       "data/CrowdFlower/GloVe/embeddings.csv",
       "data/CrowdFlower/Word2Vec/embeddings.csv",
       "data/CrowdFlower/BERT/embeddings.csv",
       "data/EmotionPush/FastText/embeddings_unsupervised.csv",
       "data/EmotionPush/FastText/embeddings_supervised.csv",
       "data/EmotionPush/GloVe/embeddings.csv",
       "data/EmotionPush/Word2Vec/embeddings.csv",
       "data/EmotionPush/BERT/embeddings.csv",
       "data/Friends/FastText/embeddings_unsupervised.csv",
       "data/Friends/FastText/embeddings_supervised.csv",
       "data/Friends/GloVe/embeddings.csv",
       "data/Friends/Word2Vec/embeddings.csv",
       "data/Friends/BERT/embeddings.csv"]
\end{lstlisting}

This was done so to facilitate the integration of new datasets or models to the analysis.
As mentioned before, the csv file contains a line for every sentence in the dataset, with the number of columns equal to the dimensionality of the model, plus one, for the label.

Every analysis script separates the embedded sentence from the label, into two structures:

\begin{itemize}
  \item $X$: contains all the embedded sentences, and is therefore of size $NxM$, where $N$ is the number of sentences in the dataset, and $M$ is the number of latent dimensions in the model.
  \item $Y$: contains all the labels of the dataset, and is of size $Nx1$.
\end{itemize}

\subsection{Correlational Analysis}\label{sub:Correlational Analysis}
The correlational analysis runs the numpy \lstinline{corcoef} \ref{TODO: https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html} algorithm between every dimension of the model, and the labels vector. A snippet of the algorithm can be seen in listing~\ref{lst:cor}

\begin{lstlisting}[caption={Corrlelation Algorithm},label=lst:cor,frame=single,numbers=left]
cors = []
for emotion in ind:
    y = (Y == emotion).astype(int)
    cor_p_sent = []
    for j in range(X.shape[1]):
        x = normalize(X[:, j].reshape(-1, 1)).reshape(-1)
        c = np.corrcoef(x, y)[1,0]
        cor_p_sent.append(c)
    cors.append(cor_p_sent)
cors = np.array(cors)
x = np.nan_to_num(cors)
\end{lstlisting}

The correlation is done between every dimension, and every emotion. Therefore, the labels vector is filtered with the selected emotion, as it can be seen on line 3. This results in a vector of size $N$ filled with zeros, except in the places where the selected emotion is the label. This ones-and-zeros vector is the reason why the dimensions vector is normalized. The latent space of every model is different. By normalizing it, we restrict the embedding values between 0 and 1, since the default normalization algorithm uses the L2 norm.
The next step, evaluating the numpy corrcoef function, returns the Pearson product-moment correlation matrix.
A matrix is formed from the correlations of every dimension, against every emotion. The resulting matrix is then of size $MxE$, where $E$ is the number of the emotions labeled in the dataset.


\subsection{Linear Dimentionality Reduction}\label{sub:Linear Dimentionality Reduction}
For a linear dimensionality reduction algorithm, PCA has been selected. The methodology here only differs from the linear correlation analysis in that a PCA transformation is preformed before examining correlations. It looks as follows:

\begin{lstlisting}[caption={PCA correlation Algorithm},label=lst:pca,frame=single]
projection = PCA().fit_transform(X)
cors = []
for emotion in ind:
    y = (Y == emotion).astype(int)
    cor_p_sent = []
    for j in range(projection.shape[1]):
        x = normalize(projection[:, j].reshape(-1, 1)).reshape(-1)
        c = np.corrcoef(x, y)[1,0]
        cor_p_sent.append(c)
    cors.append(cor_p_sent)
cors = np.array(cors)
x = np.nan_to_num(cors)
\end{lstlisting}


\subsection{Non-Linear Dimentionality Reduction}\label{sub:Non-Linear Dimentionality Reduction}
The non-linear dimensionality reduction algorithm selected was the T-distributed Stochastic Neighbor Embedding (TSNE). This uses the distribution information from the embedded sentences to search for a linearly-separable two-dimensional projection. This algorithm was selected for its relationship to visualizations. The multi-core algorithm from the MulticoreTSNE library was used~\cite{ulyanov2016tsne}.


\subsection{Clustering Analysis}\label{sub:Clustering Analysis}
The clustering algorithm used was SciPy's linkage algorithm, a part of the cluster.hierarchy library~\cite{scipy2020}. This algorithm is an aglomerative, or bottom-up clustering algorithm. It measures euclidian distance between the selected data points, and clusters them one by one. The result of this algorithm can be seen as a dendrogram plot, next to every heatmap in the results section~\ref{sec:Results}.
