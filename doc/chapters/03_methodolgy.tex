%!TEX root = ../thesis.tex
\chapter{Methodology}\label{chap:Methodology}
% Justification for the methodology

To analyze the representation of emotions in different word embeddings, this project has been divided in two main parts: Embedding and Analysis. The embedding part includes selecting the intermediate representation of the dataset, and the usage of the language model to do so. The Analysis is focused on finding the information contained in the language models, through the exploration of the mentioned intermediate representation. A high emphasis on dimensionality-reducing visualizations was done in this last part. These allow for the development of intuitions that can be further explored through statistical tests.

The process for finding information on the desired embedded structure has been divided in consecutive steps, that represent progressive steps into finding structure in a dataset. The steps are the following:

\begin{enumerate}
  \item \textbf{Correlation}: A correlation between embedded dimensions and labels.
  \item \textbf{Linear Transformation}: A correlation between the labels and a linear transformation of the embedded dimensions.
  \item \textbf{Non-Linear Transformation}: A non-linear transformation for obtaining separable clusters.
  \item \textbf{Hierarchical clustering}: Clustering the embedded dimensions, or linear transformations of these.
\end{enumerate}

These steps also correspond to the complexity of a theoretical classifier on the embedded dimension. The first one answering to the question: Can the output of the embeddings be used directly to classify the labels? The second step corresponds to asking if a linear transformation of the embedded space could yield a classifier. For example, can an LDA be applied to this dataset and embedding? The third question relates to the performance of a non-linear classifier. This is the case of a Single Layer Perceptron. Considering the embeddings can be used as an intermediate step, for a further classification, this step should already yield the results of baseline emotion classifiers. The fourth question relates to the relation of emotion and valence. Emotions tend to fit hierarchically into affective models of arousal and valence~\cite{barradas2016thesis}. By searching for a hierarchy in the representation of emotions, we should be able to trace back the representation of valence.

Separating the methodology in this way, enables a progressive approximations approach to answering the research question. It is highly unlikely that a general language model in machine learning represents emotions into a single dimension in a linear manner, but it is increasingly more likely that some correlation is found with a linear transformation of the aforementioned. In case these two approaches present no information about emotions, a hierarchical clustering can extract the intrinsic information of affect in emotions. Since previous works have already shown that affect can be represented in vector spaces, created with a linear transformation of word embeddings~\cite{hollis2016principals}, it would be contradictory to not find a hierarchical structure of emotions in this last step. If this were to happen, it would be reasonable to question the dataset and it's methodology, or the contextual information lost in the embedding process.

\section{Preliminaries}\label{sec:Preliminaries}
This research was managed as both, a research project, and a software development project.
With scientific rigor, order, and reproducibility in mind, a git repository has been setup, where not only the working environment is provided, but also the history of the project  development.

\subsection{Environment Setup}\label{sub:Environment Setup}
The environment is all required hardware and software necessary to execute this computational experiment. Here it is presented how to reproduce the same environment, to be able to reproduce the results presented. A short organizational note is also included, to keep track of the project management.

\subsubsection{Organizational}\label{subs:Organizational}
The project planning was planned for three months: March, April and May of 2020. A total of twelve weeks were divided into four equal sprints, where the four main tasks in the project were equally separated in time: Exploration and Preparation, Programming, Experiments, and Writing. The four sprints were described by tasks, further divided by sub-tasks. These were kept in track and followed by me, and both Supervisors through the Asana application.

The repository is accessible through github: \\
\url{https://github.com/abcsds/MasterThesis}

More information about the development environment can be found on section~\ref{sec:Development Environment}, in the Appendix.

\subsection{The Datasets}\label{sub:The Datasets}
Three datasets were selected to be used for this project. Here it's described how and when they were accessed, stored and embedded into the intermediate representation.

\subsubsection{Access}\label{subs:Access}
Accessing datasets to train machine learning models is not a standardized process. The developer of every dataset is in charge of the distribution method. Fortunately, two of the three datasets used in this project were distributed by the same organization: the EmotionPush, and Friends datasets were, while CrowdFlower was distributed originally by a company with the same name.

The CrowdFlower dataset was downloaded from the official CrowdFlower website in October 2019. The URL to this dataset is \url{http://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv}. As of May 20, 2020, this link still works, but the website \url{www.CrowdFlower.com} redirects to \url{www.appen.com} a company that 'collects [data] to build [\ldots] artificial intelligence systems.' This company offers access to some open source datasets, but the mentioned CrowdFlower emotion dataset is not listed there. A discussion on this is provided on chapter \ref{chap:Conclusion}.

The EmotionPush and Friends datasets were distributed as part of the EmotionX Task, which in turn is part of a set of Social NLP tasks, created by the Academia Sinica of Taiwan~\cite{chen2018emotionlines}. To access this datasets, one must register on the EmotionX 2019 website: \url{https://sites.google.com/view/emotionx2019}. Access to a google drive is then granted via email, and a zip file with both datasets can be downloaded. This dataset has been used more than once in different analysis on the internet, and it can be therefore accessed without permissions to the official method. Here, the original dataset is used.

\subsubsection{Storage}\label{subs:Storage}
The datasets were downloaded and stored under the project folder 'data'. Since every dataset is provided in different format and under different folder structures, every dataset is simply stored inside a folder with it's name.
Under the datasets folder, every selected dataset is accompanied by folders with the embedding model used to embed the dataset. Thus every dataset folder has several sub-folders. On these sub-folders, a python script called 'embed.py'. This script varies for every model and dataset. In general terms, it extracts the text and label from the dataset, embeds the text into the desired model, and stores it in a 'csv' file under the same folder.
The 'csv' file is stored under the name 'embedded.py', except for the FastText model. In this case, there are two embedding approaches, one supervised and one unsupervised. Thus the names of the FastText embedding files are 'embeddings_supervised.csv', and 'embeddings_unsupervised.csv'. Every other script creates a single 'csv' file called 'embeddings.csv'.

The file structure of the embedded files allows for exploration and experimental scripts to access the embedded data of different datasets, by building a single string with the dataset and model selected. This string must be prepended by the './data/' folder name, and appended with the 'embeddings.csv' string to generate a path that creates accessibility to the different datasets via a python coma-separated-value library, such as the built in \lstinline{csv}, or Pandas~\cite{reback2020pandas} and it's \lstinline{read_csv} function. This effectively create a data source to be used in a data pipeline. This approach was selected due to it's simplicity.

\section{Embedding}\label{sec:Embedding}
The comparison of the representation of different language models in this project requires a convergence of many different techniques. For this reason it was chosen to embed the datasets into an intermediate format, to later use them in experiments.

The embedding of the datasets is comprised of 5 steps:

\begin{enumerate}
  \item Loading model, text, and labels.
  \item Tokenizing text.
  \item Embedding every token into the model latent space.
  \item Average the given embedded words.
  \item Store the average sentence embedding.
\end{enumerate}

\subsection{Embedding Methodology}\label{sub:Embedding Methodology}

Loading text, and labels was done with either the CSV or the JSON python library, depending on the format of the data.

Tokenizing was done with Spacy's 'en_core_web_sm' model, which allows access to the tokens via an iterator on the model, and the sub-component 'text'. An small snippet showing this process is shown in~\ref{lst:spacy}. This snippet considers a model has been loaded as a dictionary on tokens.

\begin{lstlisting}[caption={Tokenizing with Spacy},label=lst:spacy,frame=single]
import spacy
nlp = spacy.load("en_core_web_sm")
for token in nlp("This is a sentence in English"):
  word_embedding = model[token.text]
\end{lstlisting}

Every token is embedded in this way, but some models might not contain some tokens. In this case, the token is simply skipped. Some tokens with relevant information can be lost with using pre-trained models that don't contain the complete vocabulary of the dataset, but it is expected, that the information distribution converge to the real distribution when large number of samples are integrated.

Once every token has been embedded into the model's latent space. A simple average is done across the tokens, keeping the dimensionality of the vector representation, and effectively creating a sentence embedding, represented in the model's latent space. This technique was selected since it's the most common method for sentence representation. With this method, the sequential nature of the tokens in the sentence is lost, in favor of providing a constant sized sentence embedding to compare between methods and datasets. Many other methods for sentence embedding that do convey the sequential nature of the word embeddings in a sentence require some extra training. The reason for this project to use average embeddings is that the pre-trained embeddings can be used exactly as they come out of the language model, and requires no extra learning.

Lastly, the sentence embeddings are stored along with the label information. For this, the CSV format was selected, due to it's interoperabiliy, and accessibility. The statistics library Pandas has an excellent csv reader, but the data can also be imported into spreadsheet software, other statistical software, or very quickly loaded on to python with the CSV library. Every CSV file contains a header on the first row. The header is composed by $N+1$ columns where $N$ is the number of latent dimensions in the model. The last column is the "Emotion" column, where the label is stored. The name of every column starts with the letter 'd', and is followed by consecutive numbers.

Since every pre-trained model is different, there were specific requirements on loading the model and embedding the tokens:

\subsection{FastText}\label{sub:FastText}
As previously mentioned, the FastText algorithm is an exception in this project, since it is NOT a pre-trained model. The model is trained based on the dataset given. This can be done in a supervised, or an unsupervised manner. Due to the two methods for the usage of the FastText python library, the process of embedding a dataset with it requires two extra text files one with a sentence per line, and a second one, which includes the label as the last word of every line, prepended by two underscores (\lstinline{__}).

In both ways of training, the language model is being trained specifically for the dataset vocabulary. For this reason, all tokens will be available in the model's vocabulary, resulting in the most complete language model. This is at the cost of representing only the topics on the dataset. This is therefore also not a general language model.

\subsection{Word2Vec}\label{sub:Word2Vec}
Word2Vec is trained in a very similar way as FastText. Therefore, the expected results are similar. Word2Vec is treated within the context of this experiments as the pre-trained equivalent of fast text. The same number of latent dimensions, and a similar training approach were used. In this case, if a word in the dataset is not contained in the Word2Vec model, it is dropped, and its analysis won't be included in the results of this project. Word2Vec was trained with a very large corpus, it is therefore considered a general language model.

The pre-trained model has been stored under the project folder \\ './models/Word2Vec/GoogleNews-vectors-negative300.bin.gz'. The Gensim python library is used to load the model in binary format without having to decompress it. This model is loaded as a dictionary. An example of this is shown in snippet~\ref{lst:load_w2v} that considers an iterator over a tokenized sentence.

\begin{lstlisting}[caption={Loading Word2Vec},label=lst:load_w2v,frame=single]
import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)
for token in tokenized_sentence:
  word_embedding = model[token]
\end{lstlisting}

\subsection{GloVe}\label{sub:GloVe}
Pre-trained GloVe models can be downloaded from the official website \url{https://nlp.stanford.edu/projects/glove/}. This model was downloaded and stored under the project folder './models/GloVe/glove.6B/glove.6B.300d.txt'. The name of this file contains two numbers: 6B is the number of words that are represented in this model, while 300d is the number of latent dimensions used to represent the vocabulary. This model has been trained for 50, 100, 200, and 300 dimensions. Since a smaller number of dimensions represents a lesser capability for representing complex language concepts~\cite{penningto2014glove}, the larger version of this model was selected. This also coincides with the number of dimensions used in Word2Vec, which makes results easier to compare.

\subsection{BERT}\label{sub:BERT}
Although BERT is a pre-trained model, it's original distribution is considered to be only partially trained. On the original paper~\cite{devlin2019bert}, a fine-tuning task-specific phase is mentioned, and generally required for the model to work best. This fine-tuning also presents a great infrastructure challenge, since some pre-trained BERT models simply wont fit into a personal computer's RAM.\@

For this reason, the pre-trained BERT embedding library \url{https://github.com/imgarylai/bert-embedding} was used. This library allows for a selection of the BERT model, and the embedding of the whole sentence, without tokenization. The result is a json-like dictionary in Python that contains both the original sentence and the embedded sentence.

To be able to run the embedding notebook, provided under the project folder 'exploration/Embedding with bert.ipynb', the following requirements should be met:
\begin{itemize}
  \item Docker $\geq19.03$
  \item NVIDIA Container Toolkit
  \item This Docker TF Image: \\ \lstinline{tensorflow/tensorflow:2.1.0-gpu-py3-jupyter}
\end{itemize}

On Linux, the a correct installation of the nvidia-docker environment would yield a successful run of the following command: \lstinline{docker run --gpus all --rm nvidia/cuda nvidia-smi}

To build the docker image for this project, one must open a terminal on the 'TF' project folder and run the following docker instruction: \lstinline{docker build -t bert .} where bert is the name of the image to be created.
Once this image has been built, docker can create containers with it. So to run the container necessary for the BERT embedding, the following command is used inside the project folder: \lstinline{docker run --gpus all -p 8888:8888 -v $(pwd):/tf -it bert}.
This last command will run a docker container, based on the "tensorflow:2.1.0-gpu-py3-jupyter" image, connect it to the localhost port 8888, and integrate the project folder to the jupyter server running on the container.

Docker is used to comply with the complex requirements of TensorFlow, CUDA, and the bert-embeddings.Once the Jupyter server is running, the notebook can be opened, and executed. The loading of the model is shown in the following snippet~\ref{lst:load_bert}:

\begin{lstlisting}[caption={Loading BERT},label=lst:load_bert,frame=single]
from bert_embedding import BertEmbedding
bert_embedding = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased')
\end{lstlisting}

Here, the selected model is shown. This is a model with 1024 latent dimensions, trained on the Wikipedia corpus, and with case sensitivity. This means that words lowercase and uppercase letters will be embedded differently.

Within the notebook, a function was created to embed the datasets. This receives three arguments: a list of the sentences, a list of the labels, and the name of the output file, as a string. The embedding function is shown here:

\begin{lstlisting}[caption={Embedding with BERT},label=lst:embed_bert,frame=single]
def embed_and_save(X, Y, outpath):
    E = np.array([np.mean(t[1], axis=0) for t in bert_embedding(X)])
    with open(outpath, 'w', newline='') as f:
        fieldnames = [f"d{i}" for i in range(len(E[0]))] + ['emotion']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for e, l in zip(E, Y):
            writer.writerow(dict({f"d{i}": ei for i, ei in enumerate(e)}, **{'emotion': l}))"
\end{lstlisting}

Running the embeddings for the datasets reported the following data:

\begin{table}[H]
  \begin{tabular}{lllll}
  Dataset                          & User              & System    & Total        & Wall        \\
  \hline
  \multicolumn{1}{l|}{CrowdFlower} & user 3h 57min 7s  & 13min 11s & 4h 10min 18s & 1h 5min 33s \\
  \multicolumn{1}{l|}{EmotionPush} & user 1h 28min 35s & 5min 25s  & 1h 34min 1s &  24min 31s   \\
  \multicolumn{1}{l|}{Friends    } & user 1h 26min 40s & 5min 3s   & 1h 31min 44s & 24min 9s
  \end{tabular}
  \caption{Run times for embedding datasets with BERT}\label{tab:rt_BERT}
\end{table}

This is much less than the 'several days' verbally reported by colleagues at the ISMLL. This might be due to the use of pre-trained models, and not running back-propagation to fine-tune the language models.

While running the embeddings, almost no GPU memory was used. This signals that the selected library is actually not making use of the GPU resources available. This also might mean that the embedding of the datasets might be much faster if a custom BERT embedding algorithm is used, that uses GPU. Since there was no task-specific learning, and the embeddings were done only once, and stored, this step is seen as inefficient in the context of this project, but should be considered for the optimization of further studies.

At the beginning of the Year 2020, the library seemed a reliable way of getting the embedding done quickly. It allowed for embedding of the complete datasets in matter of minutes. Since I had been warned BERT embeddings could take days, I saw this as a great advantage, and kept the method. Unfortunately as of May 2020, this library has been deprecated. It's unmaintained, and has requirements that might only be achievable under very specific conditions. This will not be a problem for reproduction, as long as the library is still available, and the provided docker image is used.

\section{Analysis}\label{sec:Analysis}
The python scripts to analyze the data are found under the folder './exploration', where they are numbered, and named. The order of the scripts corresponds to the progressive steps in the search for structure in the embedded spaces. The scripts are the following:

\begin{enumerate}
  \item \lstinline{01_corr.py}
  \item \lstinline{02_pca.py}
  \item \lstinline{03_tsne.py}
\end{enumerate}

The order of these scripts corresponds to the methodology proposed in this thesis. They generate the visualizations, and test the hypothesis on the data. The last step in the methodology, Clustering, has been performed in all scripts. The scripts each contain a list of strings. Every string in that list is the relative path of one of the pre-processed datasets. These strings can be commented out. In doing so, the analysis will not be run on that specific instance. The full list is declared as follows:

\begin{lstlisting}[caption={Pre-processed datasets},label=lst:datasets,frame=single]
dss = ["data/CrowdFlower/FastText/embeddings_unsupervised.csv",
       "data/CrowdFlower/FastText/embeddings_supervised.csv",
       "data/CrowdFlower/GloVe/embeddings.csv",
       "data/CrowdFlower/Word2Vec/embeddings.csv",
       "data/CrowdFlower/BERT/embeddings.csv",
       "data/EmotionPush/FastText/embeddings_unsupervised.csv",
       "data/EmotionPush/FastText/embeddings_supervised.csv",
       "data/EmotionPush/GloVe/embeddings.csv",
       "data/EmotionPush/Word2Vec/embeddings.csv",
       "data/EmotionPush/BERT/embeddings.csv",
       "data/Friends/FastText/embeddings_unsupervised.csv",
       "data/Friends/FastText/embeddings_supervised.csv",
       "data/Friends/GloVe/embeddings.csv",
       "data/Friends/Word2Vec/embeddings.csv",
       "data/Friends/BERT/embeddings.csv"]
\end{lstlisting}

This was done so to facilitate the integration of new datasets or models to the analysis.
As mentioned before, the csv file contains a line for every sentence in the dataset, with the number of columns equal to the dimensionality of the model, plus one, for the label.

Every analysis script separates the embedded sentence from the label, into two structures:

\begin{itemize}
  \item $X$: contains all the embedded sentences, and is therefore of size $N \times M$, where $N$ is the number of sentences in the dataset, and $M$ is the number of latent dimensions in the model.
  \item $Y$: contains all the labels of the dataset, and is of size $N \times 1$.
\end{itemize}

\subsection{Correlation Analysis}\label{sub:Correlation Analysis Method}
The correlation analysis runs the numpy \lstinline{corcoef}~\cite{oliphant2006numpy} algorithm between every dimension of the model, and the labels vector. The \lstinline{corcoef} algorithm runs spearman's correlotion. The equation for spearman's correlation is

\begin{equation}
  R_{X,Y} = \dfrac{\EX[(X- \mu_x) (Y - \mu_y)]}
                 {\sigma_X \sigma_Y}
\end{equation}

A snippet of the algorithm can be seen in listing~\ref{lst:cor}

\begin{lstlisting}[caption={Correlation Algorithm},label=lst:cor,frame=single,numbers=left]
cors = []
for emotion in ind:
    y = (Y == emotion).astype(int)
    cor_p_sent = []
    for j in range(X.shape[1]):
        x = normalize(X[:, j].reshape(-1, 1)).reshape(-1)
        c = np.corrcoef(x, y)[1,0]
        cor_p_sent.append(c)
    cors.append(cor_p_sent)
cors = np.array(cors)
x = np.nan_to_num(cors)
\end{lstlisting}

The correlation is done between every dimension, and every emotion. Therefore, the labels vector is filtered with the selected emotion, as it can be seen on line 3. This results in a vector of size $N$ filled with zeros, except in the places where the selected emotion is the label. This ones-and-zeros vector is the reason why the dimensions vector is normalized. The latent space of every model is different. By normalizing it, we restrict the embedding values between 0 and 1, since the default normalization algorithm uses the L2 norm.
The next step, evaluating the numpy corrcoef function, returns the Pearson product-moment correlation matrix.
A matrix is formed from the correlations of every dimension, against every emotion. The resulting matrix is then of size $M \times E$, where $E$ is the number of the emotions labeled in the dataset.


\subsection{Linear Dimensionality Reduction}\label{sub:Linear Dimentionality Reduction}
For a linear dimensionality reduction algorithm, PCA has been selected. The methodology here only differs from the linear correlation analysis in that a PCA transformation is preformed before examining correlations. It looks as follows:

\begin{lstlisting}[caption={PCA correlation Algorithm},label=lst:pca,frame=single]
projection = PCA().fit_transform(X)
cors = []
for emotion in ind:
    y = (Y == emotion).astype(int)
    cor_p_sent = []
    for j in range(projection.shape[1]):
        x = normalize(projection[:, j].reshape(-1, 1)).reshape(-1)
        c = np.corrcoef(x, y)[1,0]
        cor_p_sent.append(c)
    cors.append(cor_p_sent)
cors = np.array(cors)
x = np.nan_to_num(cors)
\end{lstlisting}


\subsection{Non-Linear Dimensionality Reduction}\label{sub:Non-Linear Dimentionality Reduction}
The non-linear dimensionality reduction algorithm selected was the \\ T-distributed Stochastic Neighbor Embedding (TSNE). This uses the distribution information from the embedded sentences to search for a linearly-separable two-dimensional projection. This algorithm was selected for its relationship to visualizations. The multi-core algorithm from the MulticoreTSNE library was used~\cite{ulyanov2016tsne}.


\subsection{Clustering Analysis}\label{sub:Clustering Analysis}
The clustering algorithm used was SciPy's single linkage algorithm, a part of the cluster.hierarchy library~\cite{scipy2020}. This algorithm is also known as Nearest Point Algorithm. For all points in the dataset, it selects the minimum distance between it, and all other. The distance equation can be expressed as:
\begin{equation}
  d(U,V) = \min(\text{dist} (U[i], V[j]))
\end{equation}
where $i$ are all points in cluster $U$ and $j$ all points in cluster $V$, and the distance is euclidiean.
This is in result, an agglomerative, or bottom-up clustering algorithm. The result of this algorithm can be seen as a dendrogram plot, next to every heat-map in the results section~\ref{sec:Results}.
