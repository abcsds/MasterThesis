%!TEX root = ../thesis.tex
\chapter{Conclusion}\label{chap:Conclusion}
The experiments presented in this project help understand some of the reasons as of why the field of emotion research is so hard to manage in precise and conicse manners. By asuming there is a model of universal emotions, to be able to generalize to all languages and populations, one can neglect the specificity of very relevant context-dependant emotions. On the other side, by creating models that adapt to populations and languages, the generalization of those specific concepts is lost.

\section{Discussion}\label{sec:Discussion}
Language models seem not to have a strong representation of the concepts of emotions as labeled in datasets, but datasets with labeled emotions have serious problem when it comes to emotions that present more than one of the emotions from the selected emotion model. In many cases, dataset labeling selects arbitrary emotion models.

Valence, on the other hand, is present on every dataset and languge model. Since every emotion of universal models of emotions fits into a dichotomical model of valence, a top to bottom hierarchical clustering can be done with the concepts that are represented in a language model, to aproach the labeling of a dataset, and even to create a model of emotion in text.

Even the most simple of the examined language models abstracts very complex concepts, and is able to discern between very fine differences. In the case of BERT, those differences can even be on the context of the text.

\subsection{On the CrowdFlower dataset}
The CrowdFlower dataset has been used on many baseline papers cited on this project, and as a main dataset for this project. Unfortunately, I noticed too late, that there is no actual paper describing its creation. The company that created it doesn't exist anymore, and the labeling seems to have partially been done automatically. Many labels seem incorrect, or were doubtably labeled out of context. This is a major problem to consider in the present project, and must be resolved in future iterations.

\subsection{Correctly identifying and representing emotions in text}
Considering the observations of the experiments in this project, suggestions as of how to create and use a model of language, or a dataset that correctly captures emotional concepts can be made.

The first comments are on the dataset. A dataset with self-contained texts is suggested. Since the message of the text must be as context independant as possible. This does not mean that a discussion, like the case of EmotionPush or Friends cannot be analyzed, but if analyzed, it must be done as a single data point, maybe with variable emotion along the discussion.
Twitter is not a bad example of a corpus for emotion detection, but retweets and responses must be excluded, and a sample across time is suggested, to avoid the influence of events.

The text of the dataset should be a single language. Different languages will require different models of emotions. Labeling, if done by humans, should be voted on, or, if multiple labels are posible, they should be counted and voted on by intensity. In this case the labels are only to be considered the "Perceived" emotion. Which labels the emotion of the reader, and no the emotion of the original author of the text.

For a dataset trying to capture the emotion of the person writing the text, the best approach is by self-reported emotions. This means that only texts that explicitly express the experience of an emotion by the writer can be taken into account. This can be combined with a text-similarity expansion of the dataset. An example of this is selecting tweets that contain the frase: 'I feel...' followed by an emotion word, or descriptor. This specific example is similar to the way Language Models learn from context. Anything that follows that sentence, can be interpreted as a self report of an internal condition. A further analysis on this example could yield interesting results, and is considered as future work.

If a model of emotions is used to select the labels for a dataset, the consideration of what an emotion, along with the interpretation and clear differences of those emotions in the specific language of the dataset must be made. A dataset that does not explicitly express the reasons to use an emotion model should not be used. Eckman's model, for example, although universal, is based on faces. If the dataset is not of faces, Eckman's model is discouraged in favor of a more complete model of emotion in laguage.

\subsection{On average representation of sentece embedding}
For this project, a sentence embedding was simply the average of it's token's embeddings. Although simplistic, this way of representing sentences is not optimal, since the sequential nature of the tokens is lost. The problem with this is that by including the secuential nature of the sentence, like when using a RNN-based model to classify, modifies in a non-linear way the vector space, and requires much more data.

\subsection{Non-separable emotions}
As seen in this project, and presumably, also known by the reader, many emotions can be experienced at the same time. The lines between what can clearly be different emotions can get blurry as their intensity increases or the context changes. A better way of representing such abstract and complex concepts could be through statistical models. Gaussian models for example, could easily describe the distribution of emotions in the vector space. This is considered a future approach to this problem.

\subsection{Emotion Words}
The word with which one describes an emotion, the emotion name, is one of the most powerful cognitive tool that a person has to understand, manage, and express their emotions. By concentrating on emotion names, machine learning models could improve their results. Contrary to this, the results of this project show that many datasets label texts including the word 'Happy' as other emotion. When using a dataset with labeled emotions, a cross analysis of the labels and their text is suggested, to avoid or at least consider these contradictions.


\section{Future Work}\label{sec:Future Work}
This project's scope is necesarily narrow. Human language is a complex phenomena, and so are human emotions. There is much to be learned. Here, some suggestions as for the next steps to consider are made.

\subsection{Multi-label datasets}
Several datasets with labeled emotions were considered and adquired for this project. Their analysis is one of the next steps to be taken. For this, datasets with multi-labeled texts are to be considered. These are specifically difficult to visualize, but could be handeled with models of emotions like the Plutchik model, which describe emotions as superpositional.


\subsection{Learning an Emotion Model}
Language models have proven to be very powerful. They abstract concepts very well, and represent those concepts with efficiency. Given the results of this thesis, I would like to create a model of emotions in text based on Language models. This requires much more text, and probably an approach closer to the self-reported datasets, but this might even be the next step in the study of human emotions. Machine Learning is a tool that lets us examine and observe human phenomena in a way that we have not ever been able to, by examining human generated data.

% New method for analysing the abstraction of concepts on pre-trained language models
Thanks to the work done in this project a new method for the creation of an emotional model can be created. A prospect model of emotions in text can be created by using explicit textual exrpessions of emotion. An example of an explicit expression of emotion is: 'I feel sad' By removing the emotional word from this sentence, we can obtain an emotion-neutral emotion context. By parsing a corpus for these specific sentences, maping them on to a pre-trained-model vector space, and reproducing a separation method like PCA, the transformation that better captures the different emotional contexts can be obtained. This transformation can latter be used for non-explicit expressions of emotion.
This model of emotions can be context and languge specific, but the methodology is not restricted to any language, or even expressions of emotion. It is in general a method for analyzing conceptualizations of pre-trained language models.

\section{Conclusion}
In hope that this project serves as a pedagogic experience for students working on NLP concept learning, and the visualization of concepts in machine learning models of language, the code, documentation, development environment, development notes, and git repository are available under the given link in the section on Organizational~\ref{subs:Organizational}. This has been done prurposely. Visualizations are available as well as interactive explorations of the datasets, in the abstract spaces for the representation of the datasets through the language models, as well as the code to reproduce them with other datasets. We hope this work is of help to anyone trying to understand concept learning in ML models of language.
