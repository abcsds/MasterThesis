%!TEX root = ../thesis.tex
\chapter{Related Work}\label{chap:Related Work}

There have been many atempts at understanding automatically learned language models, but none has been focused on understanding emotions as we do here. For this reason, similar methods from close fields of research have been used to establish a methodology. In this chapter we talk about those methods and their contribution to this project.

The embedding of emotional words is ML Language Models can be compared directly to Emotion Lexicons. In this project we make use of that one created by Mohamad and Turney through crowdsourcing~\cite{mohammad2013crowdsourcing}.

Since creating an Emotion Lexicon through crowdsourcing is a costly taks, the rest of the models use are automatic aproaches to do so. Vo and Zhang proved that an automatic approach to learning sentiment lexicons for short texts can be done through the use of emojis~\cite{vo2016count}. This method uses the intrinsic usage of emojis to express positive or negative valence in a sentence, and exploded this to expand that valence to words used in the same context.

Maas et al\. created a method to learn word vectors for sentiment analysis in 2011~\cite{maas2011learning}.

By applying machine learned automatic embeddings, the creation of word embeddings based only on text data was open as a possibility. This is also a method that later became the popular Word2Vec model~\cite{mikolov2013word2vec}. The methods used in this paper were used to create word embeddings specific for the used datasets. More about this can be read on this same chapter~\ref{sub:Selected Language Models}. The Word2Vec model proved that concepts can be abstracted by providing semantic arthimetic. This allows for functions like the substraction of concepts to obtain their root meaning.

Although Word2Vec was inarguably useful when it was created, it was also proved redundant by experiments like the one by Rothe et al\., who suggested an orthogonal transformation to word embeddings used on SemEval2015~\cite{rothe2016orthogonal}. This yielded ultradense word embeddings for affect concepts. This first example of a linear transformation on an abstract space opened the posibility of transforming the vector space to understand it.

As a way to reduce redundancies, and understand the absraction of valence, affect, and other similar concepts, Hollis et al\. further explored transformations of a word vector space by means of component analysis, thus creating models of semantics from text~\cite{hollis2016principals}. The current project heavely relies on this specific research. Under our theoretical framework, affect is a superset of emotion, and since Hollis et al\. have already found abstractions of affect in Word2Vec, we expect to find similar results.

The mentioned research has only been done with affect. Research in the field of emotion detection is scarce, and generaly doesn't fulfill the prestige or quality requirements suggested by the Masters of Science in Data Analytics. A reason for this might be the abstract nature of emotions, the outdated emotion model, the lack of scientific foundations in the creation of datasets of emotion in text, or simply the overwhelming usefulness of affect in comparison with emotion analysis.


\section{Language Models}\label{sec:Language Models}
There is an incredible amount of pre-trained Machine Learned Language models. For this project we have selected models based on the following criteria:

\begin{itemize}
  \item The model was trained with a large amount of general purpose language corpora.
  \item It represented a breakthrough in NLP tasks at the moment of its publication.
  \item The model has been reproduced, implemented, and tested in many ML language tasks.
\end{itemize}

Under this criteria, four models have been spotted as candidates for the experiment:
\begin{itemize}
  \item Word2Vec: Words to Vectors
  \item GloVe: Global Vectors for Word Representation
  \item ELMo: Embeddings from Language Models
  \item BERT: Bidirectional Encoder Representations from Transformers
\end{itemize}

Word2Vec is the result of converting large corpus into itself, by using an auto-encoder method, with help of a one-hot encoding of the corpus vocabulary~\cite{TODO}. At the time of its publication it captured much atention, mostly due to the posibility of semantic artimetic. This was tipified by the \"King - Man + Woman = Queen\" example. Due to the one-hot encoding step in the algorithm, it does not solve the problem of words with multiple meanings.

Glove is recognizable between other language models, for its linear substructures of meaning. Since it was trained on aggregated co-occurrence statistics, it captures semantic structure better than Word2Vec~\cite{TODO}. It still asigns a one-to-one representation of words and embedded vectors, so it does not solve ambiguities.

ELMo solved this last mentioned problem by analyzing context~\cite{TODO}. This was acheived by training on prediction of words in forwards and backwards passes. Even though this model solved the problem of context-dependant meaning, it was created with the premise that context in text is secuential, and it's architecture dependant on LSTMs showed this.

BERT was the first algorithm to solve this problem, by implementing a context-dependant learning, that is not based on the secuential structures. This was done with the use of Transformers. A deep learning architecture based on the atention model, that does not depend on secuential structures.

Both BERT and ELMo give different embeddings to words in different contexts, but BERT has proven better at solving language tasks. For this reason, only BERT will be used in this project.

One last model will be used as a mean of comparing results between the different models. This is FastText~\cite{TODO}. FastText is very similar to the algorithm with which word2vec was created. It creates a one hot encoding of a corpus, and creates a latent dimension through training either an autoencoder, for an unsupervised approach, or a classifier, for a supervised one. This algorithm requires traning on the corpus. Since the corpus selected on this project are relatively small, FastText provides a way to create a baseline for pretrained models, by analyzing what a basic model trained only on the corpus would look like.

\subsection{Selected Language Models}\label{sub:Selected Language Models}

Considering the prospective models, and the given criteria, the selected models for embedding the datasets are the following:
\begin{itemize}
  \item Fasttext
  \item Word2Vec
  \item GloVe
  \item BERT
\end{itemize}

\subsubsection{FastText}\label{subs:FastText}
Python's FastText library\cite{TODO} is used in this project. This provides two approaches for training the model: an unsupervised, and a supervised. The unsupervised requires a text file with one sentence per line. The algorithm is in charge of the tokenization. This of course only works in english. The supervised approach requires a similar file for the corpus, but at the end of every line, two underscores most be followed by the label of the given sentence.

\subsubsection{Word2Vec}\label{subs:Word2Vec}
Since this pre-trained model has a one-to-one correspondance between word and embedding, a dictionary can be downloaded and imported via the gensim python library~\cite{TODO}. This model has been trained with the Google News \cite{} corpus. It weights about 1.5 Gb, and has a latent space of 300 dimensions. It is supposed to be located at the url \url{https://code.google.com/archive/p/word2vec/}, but the file is not to be found. Forums on google groups for the word2vec (\url{https://groups.google.com/forum/#!topic/word2vec-toolkit/z0Aw5powUco}) point several urls where the model can be found.

\subsubsection{GloVe}\label{subs:GloVe}
This model, provided by the Standford University, is of easy access, and as Word2Vec, can be imported as a dictionary \cite{TODO}. The download can be found under \url{https://nlp.stanford.edu/projects/glove/}. This specific version selected was trained on the Wikipedia corpus, contains 6 billion words, uses 300 latent dimensions, and weights less than 1Gb.

\subsubsection{BERT}\label{subs:BERT}
The BERT model is trained not with one, but two types of tasks. The first one is masked word or sentence prediction, and a second one requires extra layers on the architecture and a fine-tunning training for task specific performance. \cite{TODO} The pre-trained model that one can get is the language model trained with the masked-language task. This model is not as easy to get, since the defoult python libraries to import BERT, require training and fine-tunning. For this reason, the bert-embeddings python library has been selected for this task.

\section{Analysis Algorithms}\label{sec:Analysis Algorithms}
Two algorithms have been chosen for dimensionality reduction:
\begin{itemize}
  \item PCA
  \item TSNE
\end{itemize}

PCA can be interpreted as a linear transformation on the input space, that yields the maximum explainability by the least amount of dimensions. While TSNE uses statistical information to maximize the distribution of information of groups, while minimizing the distribution of information within groups.

\section{Datasets}\label{sec:Datasets}
There are many datasets of \"emotion in text\" on the internet, and finding them is not a new problem. Unfortunately, the methodology and rigor for their creation cannot be easily tested. A heavy use of the paper "An analysis of annotated corpora for emotion classification in text" by Klinger in 2018 \cite{klinger2018analysis} was done. This paper not only collects information about the datasets, but also tests their valididty in the context of a text cliassifier.

\subsection{Inclusion Criteria}\label{sub:Inclusion Criteria}
To be included into these experiments, the following criteria must be met by a dataset:
\begin{itemize}
  \item The dataset must contain short labeled texts, in english.
  \item The label must be a single emotion, from an eckman-analogous emotional model.
  \item The labels must not be a reference to valence, arousal, dominance, or other affect models.
\end{itemize}
% Justification
The text to be analyzed must be in english, since the methods and language models that we are testing will not all be available in other languages. The single-single label criterion has been chosen due to the restriction of two-dimensional projections, and their visualizations as scatter plots. The label is to be exrpessed as a single color on scatter plots, and a multi-label problem would not present the effect desired  when developing the desired intuitions.

\subsection{Candidate datasets}\label{sub:Candidate datasets}
For the datasets included in Klinger's original paper~\cite{klinger2018analysis} the naming in the paper was not followed. This is due to the inconsistencies between the paper and their github repository, which (as the moment of writing this thesis) was last updated on Dec 17 2019 (commit e58d676). The dataset naming conventions used here is the same as in the document called "unified dataset of emotion in text": \url{https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets}.

Lastly, the candidate list includes the datasets mentioned, but is not restricted to them:
\begin{itemize}
  \item AffectiveText~\cite{strapparava2007semeval} % VAD
  \item AIT-2018~\cite{SemEval2018Task1} % VAD
  \item CrowdFlower % USED
  \item DailyDialogs~\cite{li2017dailydialog} %TODO: might need to be included
  \item Emotion-Cause~\cite{ghazi2015detecting} %TODO: might need to be included
  \item Emotiondata-Aman~\cite{aman2007recognizing} % VAD
  \item EmotionPush~\cite{huang2018emotionpush} % USED
  \item EmoBank~\cite{buechel2017emobank} % VAD
  \item fb-valence-arousal~\cite{preoctiuc2016modelling} % VAD
  \item Friends~\cite{chen2018emotionlines} % USED
  \item Grounded-Emotions~\cite{liu2017grounded} % I seriously don't know what's up with this ds
  \item ISEAR International Survey On Emotion Antecedents And Reactions~\cite{scherer1990international} % Format not open source
  \item Tales~\cite{alm2005emotions} % Two different annotators, two different labels.
  \item EmoInt \cite{MohammadB17starsem} %TODO: might need to be included
  \item TEC The Twitter Emotion Corpus published \cite{mohammad2012emotional} %TODO: might need to be included
  \item Electoral-Tweets \cite{mohammad2014semantic} %TODO: might need to be included
  \item SSEC The Stance Sentiment Emotion Corpus published \cite{schuff2017annotation} %TODO: might need to be included
\end{itemize}
The link to these datasets can be found under the github repository for the unified emotion datasets.
https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets
From this list, several datasets use an affective model of valence, arousal or dominance. Removing the datasets that do not explicitly comply with the inclusion criteria leaves the following:

\begin{itemize}
  \item CrowdFlower % USED
  \item DailyDialogs~\cite{li2017dailydialog} %TODO: might need to be included
  \item Emotion-Cause~\cite{ghazi2015detecting} %TODO: might need to be included
  \item fb-valence-arousal~\cite{preoctiuc2016modelling} % USED
  \item Friends~\cite{chen2018emotionlines} % USED
  \item EmoInt \cite{MohammadB17starsem} %TODO: might need to be included
  \item TEC The Twitter Emotion Corpus published \cite{mohammad2012emotional} %TODO: might need to be included
  \item Electoral-Tweets \cite{mohammad2014semantic} %TODO: might need to be included
  \item SSEC The Stance Sentiment Emotion Corpus published \cite{schuff2017annotation} %TODO: might need to be included
\end{itemize}

% What datasets were selected and why?
During the exploratory phase of this project, a subset of these datasets will be selected, giving priority to those with a more standardized, cleaner, or easier to access data.
Models

\section{Research Question}\label{sec:Research Question}

As a general research question we propose to answer the following:
When using pre-trained models for word and sentence embedding, \textbf{is the information about the emotional and affective content or context of the word or sentence represented in the vector space?}

This question can be approached in three different ways:
\begin{itemize}
  \item Is there a direct correlation between any of the dimensions of the vector space and human-labeled emotions and affect?
  \item Is there a linear transformation that will yield a direct correlation to the same human-labeled emotions?
  \item Is there a hierarchical structure that accurately represents the embedding of said labels?
\end{itemize}
