@article{hollis2016principals,
	title = {The principals of meaning: {Extracting} semantic dimensions from co-occurrence models of semantics},
	volume = {23},
	issn = {1069-9384, 1531-5320},
	shorttitle = {The principals of meaning},
	url = {http://link.springer.com/10.3758/s13423-016-1053-2},
	doi = {10.3758/s13423-016-1053-2},
	abstract = {Notable progress has been made recently on computational models of semantics using vector representations for word meaning (Mikolov, Chen, Corrado, \& Dean, 2013; Mikolov, Sutskever, Chen, Corrado, \& Dean, 2013). As representations of meaning, recent models presumably hone in on plausible organizational principles for meaning. We performed an analysis on the organization of the skip-gram model’s semantic space. Consistent with human performance (Osgood, Suci, \& Tannenbaum, 1957), the skip-gram model primarily relies on affective distinctions to organize meaning. We showed that the skip-gram model accounts for unique variance in behavioral measures of lexical access above and beyond that accounted for by affective and lexical measures. We also raised the possibility that word frequency predicts behavioral measures of lexical access due to the fact that word use is organized by semantics. Deconstruction of the semantic representations in semantic models has the potential to reveal organizing principles of human semantics.},
	language = {en},
	number = {6},
	urldate = {2020-02-24},
	journal = {Psychonomic Bulletin \& Review},
	author = {Hollis, Geoff and Westbury, Chris},
	month = dec,
	year = {2016},
	pages = {1744--1756},
	file = {Hollis and Westbury - 2016 - The principals of meaning Extracting semantic dim.pdf:/home/beto/Zotero/storage/32CVFGQ5/Hollis and Westbury - 2016 - The principals of meaning Extracting semantic dim.pdf:application/pdf}
}

@article{rothe2016orthogonal,
	title = {Ultradense {Word} {Embeddings} by {Orthogonal} {Transformation}},
	url = {http://arxiv.org/abs/1602.07572},
	abstract = {Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information – sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efﬁcient due to the compactness of the ultradense space.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1602.07572 [cs]},
	author = {Rothe, Sascha and Ebert, Sebastian and Schütze, Hinrich},
	month = may,
	year = {2016},
	note = {arXiv: 1602.07572},
	keywords = {Computer Science - Computation and Language},
	file = {Rothe et al. - 2016 - Ultradense Word Embeddings by Orthogonal Transform.pdf:/home/beto/Zotero/storage/9RZL9BIX/Rothe et al. - 2016 - Ultradense Word Embeddings by Orthogonal Transform.pdf:application/pdf}
}

@inproceedings{yu_refining_2017,
	address = {Copenhagen, Denmark},
	title = {Refining {Word} {Embeddings} for {Sentiment} {Analysis}},
	url = {http://aclweb.org/anthology/D17-1056},
	doi = {10.18653/v1/D17-1056},
	abstract = {Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning contextbased word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Liang-Chih and Wang, Jin and Lai, K. Robert and Zhang, Xuejie},
	year = {2017},
	pages = {534--539},
	file = {Yu et al. - 2017 - Refining Word Embeddings for Sentiment Analysis.pdf:/home/beto/Zotero/storage/AJ8J92XA/Yu et al. - 2017 - Refining Word Embeddings for Sentiment Analysis.pdf:application/pdf}
}

@article{maas2011learning,
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	language = {en},
	author = {Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
	year = {2011},
	pages = {9},
	file = {Maas et al. - Learning Word Vectors for Sentiment Analysis.pdf:/home/beto/Zotero/storage/8D5S4JVT/Maas et al. - Learning Word Vectors for Sentiment Analysis.pdf:application/pdf}
}

@inproceedings{vo2016count,
	address = {Berlin, Germany},
	title = {Don't {Count}, {Predict}! {An} {Automatic} {Approach} to {Learning} {Sentiment} {Lexicons} for {Short} {Text}},
	url = {http://aclweb.org/anthology/P16-2036},
	doi = {10.18653/v1/P16-2036},
	abstract = {We describe an efﬁcient neural network method to automatically learn sentiment lexicons without relying on any manual resources. The method takes inspiration from the NRC method, which gives the best results in SemEval13 by leveraging emoticons in large tweets, using the PMI between words and tweet sentiments to deﬁne the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give signiﬁcantly better accuracies on multiple languages compared to the current best methods.},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vo, Duy Tin and Zhang, Yue},
	year = {2016},
	pages = {219--224},
	file = {Vo and Zhang - 2016 - Don't Count, Predict! An Automatic Approach to Lea.pdf:/home/beto/Zotero/storage/5U9STWZU/Vo and Zhang - 2016 - Don't Count, Predict! An Automatic Approach to Lea.pdf:application/pdf}
}

@article{devlin2019pretraining,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/home/beto/Zotero/storage/6MHC7XVS/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@inproceedings{penningto2014glove,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/home/beto/Zotero/storage/UWZU2PEJ/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf}
}

@article{mohammad2013crowdsourcing,
	title = {Crowdsourcing a {Word}-{Emotion} {Association} {Lexicon}},
	url = {http://arxiv.org/abs/1308.6297},
	abstract = {Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word–emotion and word–polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotionannotation questions, and show that asking if a term is associated with an emotion leads to markedly higher inter-annotator agreement than that obtained by asking if a term evokes an emotion.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1308.6297 [cs]},
	author = {Mohammad, Saif M. and Turney, Peter D.},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.6297},
	keywords = {Computer Science - Computation and Language},
	file = {Mohammad and Turney - 2013 - Crowdsourcing a Word-Emotion Association Lexicon.pdf:/home/beto/Zotero/storage/DG7QGQWN/Mohammad and Turney - 2013 - Crowdsourcing a Word-Emotion Association Lexicon.pdf:application/pdf}
}

@article{mikolov2013word2vec,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	language = {en},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	pages = {9},
	file = {Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:/home/beto/Zotero/storage/K77PMGQP/Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@article{russell2003construction,
	title = {Core affect and the psychological construction of emotion.},
	volume = {110},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.110.1.145},
	doi = {10.1037/0033-295X.110.1.145},
	language = {en},
	number = {1},
	urldate = {2020-02-24},
	journal = {Psychological Review},
	author = {Russell, James A.},
	year = {2003},
	pages = {145--172},
	file = {Russell - 2003 - Core affect and the psychological construction of .pdf:/home/beto/Zotero/storage/2WS3HWQF/Russell - 2003 - Core affect and the psychological construction of .pdf:application/pdf}
}

@inproceedings{klinger2018analysis,
  title={An analysis of annotated corpora for emotion classification in text},
  author={Klinger, Roman and others},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={2104--2119},
  year={2018}
}

@inproceedings{strapparava2004wordnet,
  title={Wordnet affect: an affective extension of wordnet.},
  author={Strapparava, Carlo and Valitutti, Alessandro and others},
  booktitle={Lrec},
  volume={4},
  number={1083-1086},
  pages={40},
  year={2004},
  organization={Citeseer}
}

@inproceedings{vaswani2017transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Datasets
@inproceedings{strapparava2007semeval,
  title={Semeval-2007 task 14: Affective text},
  author={Strapparava, Carlo and Mihalcea, Rada},
  booktitle={Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
  pages={70--74},
  year={2007}
}

@InProceedings{SemEval2018Task1,
 author = {Mohammad, Saif M. and Bravo-Marquez, Felipe and Salameh, Mohammad and Kiritchenko, Svetlana},
 title = {SemEval-2018 {T}ask 1: {A}ffect in Tweets},
 booktitle = {Proceedings of International Workshop on Semantic Evaluation (SemEval-2018)},
 address = {New Orleans, LA, USA},
 year = {2018}}

@article{li2017dailydialog,
 title={Dailydialog: A manually labelled multi-turn dialogue dataset},
 author={Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi},
 journal={arXiv preprint arXiv:1710.03957},
 year={2017}
}

@inproceedings{ghazi2015detecting,
  title={Detecting emotion stimuli in emotion-bearing sentences},
  author={Ghazi, Diman and Inkpen, Diana and Szpakowicz, Stan},
  booktitle={International Conference on Intelligent Text Processing and Computational Linguistics},
  pages={152--165},
  year={2015},
  organization={Springer}
}

@inproceedings{aman2007recognizing,
  title={Recognizing emotions in text},
  author={Aman, Saima},
  booktitle={Masters Abstracts International},
  volume={46},
  number={03},
  year={2007},
  organization={Citeseer}
}

@inproceedings{huang2018emotionpush,
  title={Emotionpush: Emotion and response time prediction towards human-like chatbots},
  author={Huang, Chieh-Yang and Ku, Lun-Wei},
  booktitle={2018 IEEE Global Communications Conference (GLOBECOM)},
  pages={206--212},
  year={2018},
  organization={IEEE}
}

@inproceedings{buechel2017emobank,
  title={Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis},
  author={Buechel, Sven and Hahn, Udo},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={578--585},
  year={2017}
}

@inproceedings{preoctiuc2016modelling,
  title={Modelling valence and arousal in facebook posts},
  author={Preo{\c{t}}iuc-Pietro, Daniel and Schwartz, H Andrew and Park, Gregory and Eichstaedt, Johannes and Kern, Margaret and Ungar, Lyle and Shulman, Elisabeth},
  booktitle={Proceedings of the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis},
  pages={9--15},
  year={2016}
}

@article{chen2018emotionlines,
  title={Emotionlines: An emotion corpus of multi-party conversations},
  author={Chen, Sheng-Yeh and Hsu, Chao-Chun and Kuo, Chuan-Chun and Ku, Lun-Wei and others},
  journal={arXiv preprint arXiv:1802.08379},
  year={2018}
}

@inproceedings{liu2017grounded,
  title={Grounded emotions},
  author={Liu, Vicki and Banea, Carmen and Mihalcea, Rada},
  booktitle={2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)},
  pages={477--483},
  year={2017},
  organization={IEEE}
}

@misc{scherer1990international,
  title={International survey on emotion antecedents and reactions (isear)},
  author={Scherer, KR and Wallbott, H},
  year={1990}
}

@inproceedings{alm2005emotions,
  title={Emotions from text: machine learning for text-based emotion prediction},
  author={Alm, Cecilia Ovesdotter and Roth, Dan and Sproat, Richard},
  booktitle={Proceedings of the conference on human language technology and empirical methods in natural language processing},
  pages={579--586},
  year={2005},
  organization={Association for Computational Linguistics}
}

@InProceedings{MohammadB17starsem,
	Title={Emotion Intensities in Tweets},
	author={Mohammad, Saif M. and Bravo-Marquez, Felipe},
	booktitle={Proceedings of the sixth joint conference on lexical and computational semantics (*Sem)},
	address = {Vancouver, Canada},
	year={2017}
}

@inproceedings{mohammad2012emotional,
  title={\# Emotional tweets},
  author={Mohammad, Saif M},
  booktitle={Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},
  pages={246--255},
  year={2012},
  organization={Association for Computational Linguistics}
}

@inproceedings{mohammad2014semantic,
  title={Semantic role labeling of emotions in tweets},
  author={Mohammad, Saif and Zhu, Xiaodan and Martin, Joel},
  booktitle={Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
  pages={32--41},
  year={2014}
}

@inproceedings{schuff2017annotation,
  title={Annotation, modelling and analysis of fine-grained emotions on a stance and sentiment detection corpus},
  author={Schuff, Hendrik and Barnes, Jeremy and Mohme, Julian and Pad{\'o}, Sebastian and Klinger, Roman},
  booktitle={Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
  pages={13--23},
  year={2017}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction

@article{kalachanis2015hippocrates,
  title={The Hippocratic view on humors and human temperament},
  author={Kalachanis, Konstantinos and Michailidis, Ioannis E},
  journal={European Journal of Social Behaviour},
  volume={2},
  number={2},
  pages={1--5},
  year={2015}
}

@article{irwin1947galen,
  title={Galen on the temperaments},
  author={Irwin, James R},
  journal={The Journal of general psychology},
  volume={36},
  number={1},
  pages={45--64},
  year={1947},
  publisher={Taylor \& Francis}
}

@book{darwin1872emotions,
  title={The expression of the emotions in man and animals},
  author={Darwin, Charles and Prodger, Phillip},
  year={1872},
  publisher={Oxford University Press, USA}
}

@article{ekman1992basic,
  title={Are there basic emotions?},
  author={Ekman, Paul},
  year={1992},
  publisher={American Psychological Association}
}

@article{ekman1997universal,
  title={Universal facial expressions of emotion},
  author={Ekman, Paul and Keltner, Dacher},
  journal={Segerstrale U, P. Molnar P, eds. Nonverbal communication: Where nature meets culture},
  pages={27--46},
  year={1997}
}

@article{ekman1999facial,
  title={Facial expressions},
  author={Ekman, Paul},
  journal={Handbook of cognition and emotion},
  volume={16},
  number={301},
  pages={e320},
  year={1999},
  publisher={New York}
}

@book{picard2000affective,
  title={Affective computing},
  author={Picard, Rosalind W},
  year={2000},
  publisher={MIT press}
}

@book{plutchik2013measurement,
  title={The measurement of emotions},
  author={Plutchik, Robert and Kellerman, Henry},
  volume={4},
  year={2013},
  publisher={Academic Press}
}

@book{feldman2014constructed,
  title={The psychological construction of emotion},
  author={Feldman Barrett, Lisa and Russell, James A},
  year={2014},
  publisher={Guilford Publications}
}

@book{corson1996fields,
  title={Using English words},
  author={Corson, D},
  year={1996},
  publisher={Springer Science \& Business Media}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Images
