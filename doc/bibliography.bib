@article{hollis_principals_2016,
	title = {The principals of meaning: {Extracting} semantic dimensions from co-occurrence models of semantics},
	volume = {23},
	issn = {1069-9384, 1531-5320},
	shorttitle = {The principals of meaning},
	url = {http://link.springer.com/10.3758/s13423-016-1053-2},
	doi = {10.3758/s13423-016-1053-2},
	abstract = {Notable progress has been made recently on computational models of semantics using vector representations for word meaning (Mikolov, Chen, Corrado, \& Dean, 2013; Mikolov, Sutskever, Chen, Corrado, \& Dean, 2013). As representations of meaning, recent models presumably hone in on plausible organizational principles for meaning. We performed an analysis on the organization of the skip-gram model’s semantic space. Consistent with human performance (Osgood, Suci, \& Tannenbaum, 1957), the skip-gram model primarily relies on affective distinctions to organize meaning. We showed that the skip-gram model accounts for unique variance in behavioral measures of lexical access above and beyond that accounted for by affective and lexical measures. We also raised the possibility that word frequency predicts behavioral measures of lexical access due to the fact that word use is organized by semantics. Deconstruction of the semantic representations in semantic models has the potential to reveal organizing principles of human semantics.},
	language = {en},
	number = {6},
	urldate = {2020-02-24},
	journal = {Psychonomic Bulletin \& Review},
	author = {Hollis, Geoff and Westbury, Chris},
	month = dec,
	year = {2016},
	pages = {1744--1756},
	file = {Hollis and Westbury - 2016 - The principals of meaning Extracting semantic dim.pdf:/home/beto/Zotero/storage/32CVFGQ5/Hollis and Westbury - 2016 - The principals of meaning Extracting semantic dim.pdf:application/pdf}
}

@article{rothe_ultradense_2016,
	title = {Ultradense {Word} {Embeddings} by {Orthogonal} {Transformation}},
	url = {http://arxiv.org/abs/1602.07572},
	abstract = {Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information – sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efﬁcient due to the compactness of the ultradense space.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1602.07572 [cs]},
	author = {Rothe, Sascha and Ebert, Sebastian and Schütze, Hinrich},
	month = may,
	year = {2016},
	note = {arXiv: 1602.07572},
	keywords = {Computer Science - Computation and Language},
	file = {Rothe et al. - 2016 - Ultradense Word Embeddings by Orthogonal Transform.pdf:/home/beto/Zotero/storage/9RZL9BIX/Rothe et al. - 2016 - Ultradense Word Embeddings by Orthogonal Transform.pdf:application/pdf}
}

@inproceedings{yu_refining_2017,
	address = {Copenhagen, Denmark},
	title = {Refining {Word} {Embeddings} for {Sentiment} {Analysis}},
	url = {http://aclweb.org/anthology/D17-1056},
	doi = {10.18653/v1/D17-1056},
	abstract = {Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning contextbased word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Liang-Chih and Wang, Jin and Lai, K. Robert and Zhang, Xuejie},
	year = {2017},
	pages = {534--539},
	file = {Yu et al. - 2017 - Refining Word Embeddings for Sentiment Analysis.pdf:/home/beto/Zotero/storage/AJ8J92XA/Yu et al. - 2017 - Refining Word Embeddings for Sentiment Analysis.pdf:application/pdf}
}

@article{maas_learning_2011,
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	language = {en},
	author = {Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
	year = {2011},
	pages = {9},
	file = {Maas et al. - Learning Word Vectors for Sentiment Analysis.pdf:/home/beto/Zotero/storage/8D5S4JVT/Maas et al. - Learning Word Vectors for Sentiment Analysis.pdf:application/pdf}
}

@inproceedings{vo_dont_2016,
	address = {Berlin, Germany},
	title = {Don't {Count}, {Predict}! {An} {Automatic} {Approach} to {Learning} {Sentiment} {Lexicons} for {Short} {Text}},
	url = {http://aclweb.org/anthology/P16-2036},
	doi = {10.18653/v1/P16-2036},
	abstract = {We describe an efﬁcient neural network method to automatically learn sentiment lexicons without relying on any manual resources. The method takes inspiration from the NRC method, which gives the best results in SemEval13 by leveraging emoticons in large tweets, using the PMI between words and tweet sentiments to deﬁne the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give signiﬁcantly better accuracies on multiple languages compared to the current best methods.},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vo, Duy Tin and Zhang, Yue},
	year = {2016},
	pages = {219--224},
	file = {Vo and Zhang - 2016 - Don't Count, Predict! An Automatic Approach to Lea.pdf:/home/beto/Zotero/storage/5U9STWZU/Vo and Zhang - 2016 - Don't Count, Predict! An Automatic Approach to Lea.pdf:application/pdf}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/home/beto/Zotero/storage/6MHC7XVS/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/home/beto/Zotero/storage/UWZU2PEJ/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf}
}

@article{mohammad_crowdsourcing_2013,
	title = {Crowdsourcing a {Word}-{Emotion} {Association} {Lexicon}},
	url = {http://arxiv.org/abs/1308.6297},
	abstract = {Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word–emotion and word–polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotionannotation questions, and show that asking if a term is associated with an emotion leads to markedly higher inter-annotator agreement than that obtained by asking if a term evokes an emotion.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1308.6297 [cs]},
	author = {Mohammad, Saif M. and Turney, Peter D.},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.6297},
	keywords = {Computer Science - Computation and Language},
	file = {Mohammad and Turney - 2013 - Crowdsourcing a Word-Emotion Association Lexicon.pdf:/home/beto/Zotero/storage/DG7QGQWN/Mohammad and Turney - 2013 - Crowdsourcing a Word-Emotion Association Lexicon.pdf:application/pdf}
}

@article{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	language = {en},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	pages = {9},
	file = {Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:/home/beto/Zotero/storage/K77PMGQP/Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@article{russell_core_2003,
	title = {Core affect and the psychological construction of emotion.},
	volume = {110},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.110.1.145},
	doi = {10.1037/0033-295X.110.1.145},
	language = {en},
	number = {1},
	urldate = {2020-02-24},
	journal = {Psychological Review},
	author = {Russell, James A.},
	year = {2003},
	pages = {145--172},
	file = {Russell - 2003 - Core affect and the psychological construction of .pdf:/home/beto/Zotero/storage/2WS3HWQF/Russell - 2003 - Core affect and the psychological construction of .pdf:application/pdf}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Images
